{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26abed3b-efbf-4057-b0bd-5e4f85beeaf6",
   "metadata": {},
   "source": [
    "### Reinforcement Learning\n",
    "<p><b>Reinforcement Learning (RL)</b> is a type of <b>Machine Learning</b> where an <b>agent</b> learns to make decision by <b>interacting with an environment</b>, aiming to maximize some notion of <b>cumulative rewards.</b></p>\n",
    "It is inspired by behavioral psychology similar to how humans or animals learn from <b>trial and error.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef681e-9e16-4449-a207-fbd22f501b78",
   "metadata": {},
   "source": [
    "### Why Reinforcement Learning\n",
    "#### Use Case Where RL Shines:-\n",
    "1. Games (ex. ludo, chess, atari etc)\n",
    "2. Robotics (Robot walking, talking etc)\n",
    "3. Self-Driving cars\n",
    "4. Recommender Systems\n",
    "5. Automated Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611ac93-e358-49fc-bab9-52cf798a579c",
   "metadata": {},
   "source": [
    "#### Why not Just Supervised/Unsupervised Learning?\n",
    "- Traditional ML\n",
    "  1. Needs Labeled Data\n",
    "  2. One-Shot Prediction\n",
    "  3. Passive Model\n",
    "  4. No delay rewards\n",
    "- Reinforcement Learning\n",
    "  1. Learns through interaction\n",
    "  2. Sequenial Decision Making\n",
    "  3. Active Learning Agent (Agentic AI)\n",
    "  4. Handles Delayed Feedback (works on rewards)<br>\n",
    "<b>In RL, the agent learns not just \"What is Right?\" but \"What is right over time\".</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ab14e-f034-498e-9b1b-06532c87923c",
   "metadata": {},
   "source": [
    "#### Elements of Reinforcement Learning\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Element</th>\n",
    "        <th>Description</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Agent</th>\n",
    "        <td>The Learner or Decision Maker</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Environment</th>\n",
    "        <td>Where the agent operates</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>State (S)</th>\n",
    "        <td>Current Situation the agent is in</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Action (A)</th>\n",
    "        <td>What the agent can do</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Reward (R)</th>\n",
    "        <td>Feedback from the environment</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Policy (PIE)</th>\n",
    "        <td>Strategy used by the agent to decide actions</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Value (V)</th>\n",
    "        <td>Expected long-term reward from a state</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>Q-Value (Q)</th>\n",
    "        <td>Expected reward for taking an action in a state</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e6b3b-0031-4662-8548-633ae258d66b",
   "metadata": {},
   "source": [
    "#### Exploration VS Exploitation Dilemma\n",
    "<p>This is the <b>core challenge</b> in RL</p>\n",
    "<p><b>Exploration: </b>Try new actions to discover better rewards.</p>\n",
    "<p><b>Exploitation: </b>Use known actions that give high rewards.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ad3b5-aefd-4ed3-8ef4-51538790ca01",
   "metadata": {},
   "source": [
    "#### Epsilon Greedy Theorem / Algorithm\n",
    "- Exploration -> In a small unit 0.01 - 0.2.\n",
    "- Exploitation -> Will be 0.02 - 1.\n",
    "- Win Probability +1 as a reward and -1 or 0 as a lose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac62e1-4db4-4c11-94df-3668c9ad42d1",
   "metadata": {},
   "source": [
    "#### Algorithm\n",
    "if random () < epsilon: \n",
    "    <pre>action = random_action()</pre>\n",
    "else:\n",
    "    <pre>action = action_with_max_Q_Value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aca95b6c-6883-4bc1-9b3e-f3e20376a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toss -> Heads, Tails\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5899c11-4543-445d-b924-7e3ae2af3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q_Values\n",
    "q = [ 0, 0 ]\n",
    "#20% Explore\n",
    "epsilon = 0.2\n",
    "#Learning Rate\n",
    "alpha = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44ca91c4-1bf8-496d-8dd1-b4cc21004ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_Values : [0.2330464609556261, 0.2975221392172274]\n",
      "Best Case : Tails\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    coins = 0 if random.random() < 0.7 else 1  #Let 70% is heads\n",
    "    #Greedy\n",
    "    if random.random() < epsilon:\n",
    "        guess = random.randint(0,1)\n",
    "    else:\n",
    "        guess = 0 if q[0] > q[1] else 1\n",
    "    reward = 1 if coins==guess else 0\n",
    "    q[guess] = q[guess] + alpha*(reward-q[guess])\n",
    "print(\"Q_Values :\", q)\n",
    "print(\"Best Case :\",'Heads' if q[0] > q[1] else 'Tails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaedea9-6f3c-4f88-9bf9-57c5cdda3a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
